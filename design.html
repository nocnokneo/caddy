<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>Caddy: Design</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Caddy
   </div>
   <div id="projectbrief">A 2005 Roborodentia entry with vision and path planning capability</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.1.2 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Macros</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Design </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="team_process"></a>
Collaborative Team Process</h1>
<p>The team for this project was formed from interested members of the the <a href="http://robotics.calpoly.edu/">Cal Poly Robotics Club</a>.</p>
<p>To organize the tasks and identify critical paths in the short project time line, we used <a href="http://www.ganttproject.biz/">GanttProject</a> to create a Gantt chart.</p>
<p>For code control and collaboration we used Concurrent Versions System (CVS). Since this project had a competitive nature, we chose to setup and host our own private CVS server rather than use a free, Internet-based hosting service.</p>
<p>Between face-to-face team meetings we used Drupal to host a private forum for discussing ideas, sharing progress, etc.</p>
<p>The inline code documentation and this project report were both managed using <a href="http://doxygen.org">Doxygen</a>. Keeping the documentation in plain text and means that the documentation can be version controlled the very same way as the source code. The documentation also tends to stay more up to date since it can be more conveniently updated at the same time as the source code.</p>
<h1><a class="anchor" id="system_arch"></a>
System Architecture</h1>
<p>When taking a holistic look at the project goals and requirements, it is clear that a camera-based vision system can satisfy line following, junction detection and ball-finding requirements. The image processing required for these task can all be done with a camera that is low resolution, low power, and low cost. The ball finding task, in particular, has few other options that are both low cost and simple to implement. The <a href="http://cmucam.org/projects/cmucam2">CMUcam2</a> developed by students at Carnegie Mellon University and sold through distributors as a packaged vision system, met our needs well.</p>
<p>Since the CMUcam can handle all the computationally intensive image processing as well as drive 5 servo control outputs, our requirements on the main microcontroller were fairly relaxed. The most computationally demanding task for the main microcontroller is the shortest path algorithm, but with a relatively small map even this could be handled by a low-end microcontroller.</p>
<div class="image">
<img src="electronics_block_diagram_dia.png" alt="electronics_block_diagram_dia.png"/>
<div class="caption">
Electronics block diagram</div></div>
<h1><a class="anchor" id="electrical_design"></a>
Electrical Design</h1>
<h2><a class="anchor" id="power_motor"></a>
Power Regulation and Motor Controller</h2>
<p>We opted to design and build our own power regulation and motor control circuitry over purchasing one of the more expensive off-the-shelf solutions. To ensure that we were not debugging software and electrical issues at the same time, we made sure to include robust regulation and decouple in the design. Our power sub-component power needs were:</p>
<ul>
<li>+5V regulated power for the ATMega32 and the rest of the electronics </li>
<li>+6-15V unregulated power for the CMUcam2 </li>
<li>+6V for each motor, controllable via logic-level signal</li>
</ul>
<p>Unregulated voltage was connected to the CMUcam2 (it had a built-in regulator) and to the motors. Although not ideal, connecting the motors to unregulated power meant we could save on cost by using a smaller, cheaper voltage regulator. We opted for a linear regulator to supply the +5V to the ATMega32 microcontroller.</p>
<p>In the choice between a linear regulator and a switching regulator, a linear was chosen over a switching regulator for the following reasons:</p>
<ul>
<li>Lower output noise - We wanted to take every precaution we could to ensure the EMF voltage generated by the motors did not affect the electronics.</li>
</ul>
<ul>
<li>Simpler to implement - Switching regulators typically require more passive components to filter the inherently higher noise they generate.</li>
</ul>
<ul>
<li>Cheaper</li>
</ul>
<p>Switching regulators are more efficient, however any efficiency gains would be dwarfed in comparison with the power demands of the unregulated components (DC motors and CMUcam2). The circuit diagram for our implementation is shown below:</p>
<div class="image">
<img src="power_sch.png" alt="power_sch.png"/>
<div class="caption">
Power regulation circuit</div></div>
<p>To control the motors via digital signal we used an H-bridge circuit. For added protection of the electronics from the back-EMF voltage of the motors, additional diodes were connected as shown in the schematic below.</p>
<div class="image">
<img src="motor_control_sch.png" alt="motor_control_sch.png"/>
<div class="caption">
Motor control circuit</div></div>
<p>The power regulation and motor control circuits were fabricated together on a single board using a copper-plated board, etching solution, and a laser print out of the circuit layout. We made sure to use polarized headers for all the connections to avoid making any reverse polarity mistakes — though we still managed to make a couple!.</p>
<h2><a class="anchor" id="wheel_encoders"></a>
Wheel Encoders</h2>
<div class="image">
<img src="mounted_encoders.png" alt="mounted_encoders.png"/>
<div class="caption">
Reflective IR sensors mounted on a bracket inside the left drive wheel</div></div>
<p>The maneuvers needed at junctions and for the bonus ball pick up sequences needed to be accurate and repeatable. To achieve this we use a simple differential drive system with encoders on each drive wheel. The encoders were made up of reflective IR sensors pointed at black and white patterned disks pasted to the inside edge of each drive wheel. <a class="el" href="citelist.html#CITEREF_walters_dead_reckoning_2000">[2]</a></p>
<div class="image">
<img src="encoder_disc.png" alt="encoder_disc.png"/>
<div class="caption">
48 segment pattern for reflective IR wheel encoders</div></div>
<p>The reflective IR sensors emit infrared light and measure the amount of infrared light that is reflected back with a photodiode. Since the white segments of the pattern reflect more light than the black segments, rotations of the wheel can be detected by thresholding the measured IR signal level. To guard against "chattering" when signals close to the threshold are applied a dual-threshold Schmitt trigger is needed. The precision of this wheel encoder system is equal to about one segment arc angle — 7.5° for a 48 segment pattern.</p>
<p>The P5587 reflective IR sensor package chosen for Caddy was designed for exactly this type of high contrast edge detection and so came with built in Schmitt triggers and a logic-level output that could be easily read by a GPIO input port on the microcontroller.</p>
<h2><a class="anchor" id="ir_break_beam"></a>
IR Break Beam</h2>
<p>In order to capture ground balls, Caddy needed a way to detect when a ball was within the grasp of its lift mechanism.</p>
<p>We originally planned to use the centroid tracking feature of the camera since the camera would always be facing down for line tracking during the times it needed to detect ground balls. This turned out to be difficult mainly due to the limitations of the camera. When the camera is configured to track a black line both the glare from the overhead lighting and the randomly placed ground have the same effect on what the camera detects – a gap in the line. We tried to simply change modes whenever a gap was detected, determine if the gap was due to a glare or a due to a ball, and then act accordingly. Unfortunately, the CMUcam did not handle rapid mode and parameter changes well, taking too long to switch from one mode to another. This lead to a failure in our carefully tuned PID line tracking algorithm which relied on frequent, regular updates over time. We considered and experimented with some ways of solving this problem but none were the quick, elegant solution we were looking for.</p>
<p>With a fast approaching deadline, we opted for a quick, but less elegant, solution. We installed an IR break beam sensor looking across the front of the lift arm so that when the arm was down and fully open a ground ball would break the beam as is passed under the arm.</p>
<h2><a class="anchor" id="servo_reverser"></a>
Servo Reverser</h2>
<p>The mechanical design of Caddy required 6 servos:</p>
<ul>
<li>Ball pickup, left side </li>
<li>Ball pickup, right side </li>
<li>Boom control </li>
<li>Ball hopper </li>
<li>Tilt action </li>
<li>Pan action</li>
</ul>
<p>This meant that the original plan to use the five servo control outputs of the CMUcam would be inadequate.</p>
<p>The following approaches were considered for accommodating the 6th servo output:</p>
<ul>
<li><b>Mechanical:</b> Modify the mechanical design so that the ball pickup mechanism could be controlled by just one high-torque servo. The lift mechanism had already been iteratively refined to the point that it could be actuated by just one mechanical motion. Going this route would likely mean some major rework of the mechanical design - not ideal since we were otherwise happy with the mechanics of the robot.</li>
</ul>
<ul>
<li><b>Software:</b> Use some of the extra pins on the ATmega32 to generate a servo PWM signal. Unfortunately we were already using the two PWM peripherals on the ATmega32 so we would have to do this in software. We had limited timer resources on our chip and weren't sure how we might need to use them in the future so this was not an ideal solution.</li>
</ul>
<ul>
<li><b>Electrical:</b> Leverage the fact that the 2 servos controlling the ball pickup were the same signal, 180 degrees out of phase. This seemed like a perfect application for a simple 555 timer circuit.</li>
</ul>
<p>Searching the Internet, we found we were not the only ones to think of using a 555 timer to create a servo reverser. Using the well documented plans from C. Dane <a class="el" href="citelist.html#CITEREF_dane_the_reverser">[1]</a> we fabricated a board the size of a postage stamp.</p>
<h1><a class="anchor" id="software_architecture"></a>
Software Architecture and Algorithms</h1>
<h2><a class="anchor" id="computing_platform"></a>
Computing Platform</h2>
<p>For our our computing platform we chose an ATmega32 microcontroller from Atmel's 8-bit AVR line of microcontrollers because it was C-programmable with free open-source tools and because we had a readily available development board, the ERE EMBMega32.</p>
<div class="image">
<img src="ere_embmega32.png" alt="ere_embmega32.png"/>
<div class="caption">
EMBMega32 development board from ERE CO.,LTD</div></div>
<h2><a class="anchor" id="line_tracking"></a>
PID Line Tracking</h2>
<p>To track the black electrical tape line, we implemented a proportional–integral–derivative (PID) controller. In PID theory, the output of a PID controller, <img class="formulaInl" alt="$c(t)$" src="form_0.png"/>, is defined as:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ c(t) = P_E e(t) + P_I \int e(t) dt + P_D \frac{de}{dt} \]" src="form_8.png"/>
</p>
<p>Where <img class="formulaInl" alt="$ e(t) $" src="form_2.png"/> is some error function and <img class="formulaInl" alt="$ P_E $" src="form_3.png"/>, <img class="formulaInl" alt="$ P_I $" src="form_4.png"/>, and <img class="formulaInl" alt="$ P_D $" src="form_5.png"/> are adjustment coefficients for the observed error, the integrated error and the derivative of the error, respectively. We define our error term:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ e(t) = \frac{dx}{dt} \]" src="form_9.png"/>
</p>
<p>By substitution, we get:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ c(t) = P_E \frac{dx}{dt} + P_I x(t) + P_D \frac{d^2x}{dt^2} \]" src="form_10.png"/>
</p>
<p>Broken down and interpreted for the task of line tracking, these terms are:</p>
<ul>
<li><img class="formulaInl" alt="$ P_E \frac{dx}{dt} \leftarrow $" src="form_12.png"/> How fast are we drifting from the center line? </li>
<li><img class="formulaInl" alt="$ P_I x(t) \leftarrow $" src="form_13.png"/> How far are we from from the center line? </li>
<li><img class="formulaInl" alt="$ P_D \frac{d^2x}{dt^2} \leftarrow $" src="form_14.png"/> How fast is the drift rate accelerating?</li>
</ul>
<p>Provided that there is a way to measure or compute each of these terms, this is a more robust form of the equation because it eliminates the integral term which can cause problems due to accumulated error.</p>
<p>The figure below shows how the camera was used to measure <img class="formulaInl" alt="$ P_E \frac{dx}{dt} $" src="form_15.png"/>:</p>
<div class="image">
<img src="line_tracking_dia.png" alt="line_tracking_dia.png"/>
<div class="caption">
Diagram of line tracking geometry (not to scale)</div></div>
<p>The drift rate is the slope of the black line with respect to the center line of the robot. For points <img class="formulaInl" alt="$ P_1 = (x_1,y_1) $" src="form_29.png"/> and <img class="formulaInl" alt="$ P_2 = (x_2,y_2) $" src="form_30.png"/> from the diagram above we define:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \frac{dx}{dt} = \frac{y_2-y_1}{x_2-x_1} \]" src="form_16.png"/>
</p>
<p>And for point <img class="formulaInl" alt="$ P_3 = (x_3,y_3) $" src="form_31.png"/> with constant, measurable value for <img class="formulaInl" alt="$ y_3 $" src="form_21.png"/> and for constant, measurable line center <img class="formulaInl" alt="$ x_{center} $" src="form_26.png"/></p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ x_3 = x_{center} - \frac{dx}{dt} (y_3 - y_1) + x_1; \]" src="form_28.png"/>
</p>
<p>Lastly, by storing the previously computed value of <img class="formulaInl" alt="$ \frac{dx}{dt} $" src="form_24.png"/>, we can compute the third term:</p>
<p class="formulaDsp">
<img class="formulaDsp" alt="\[ \frac{d^2x}{dt^2} = \frac{dx}{dt} - \frac{dx}{dt}_{previous} \]" src="form_25.png"/>
</p>
<p>The coefficients for each of these terms were determined by trial and error using a tethered remote and stored persistently in EEPROM.</p>
<h2><a class="anchor" id="maneuvering"></a>
Maneuvering</h2>
<p>When turning our bot by a certain number of ticks, we experienced overshoot despite actively applying DC motor braking. We addressed the problem with the following software solution.</p>
<p>After turning for the desired number of ticks, we applied braking and counted the number of excess ticks that occurred from the instant braking was commanded. After a fixed delay, we drove the wheels in the opposite direction for that same number of ticks.</p>
<p>This worked well for the most part, however, with different battery charges, turn amounts, and turn types, the amount of time to brake was never the same. If we did not brake the motors for a long enough delay, our bot would stop counting excess ticks and begin to drive the motors in the opposite direction, too soon. With our unsophisticated encoders that cannot detect the direction of wheel motion this resulted in "reverse ticks" being counted before the wheel had actually started moving in the reverse direction.</p>
<h2><a class="anchor" id="localization"></a>
Localization</h2>
<p>While the locations of the ground balls were not known a priori, the map of the course was. We defined the course as a connected undirected graph with 42 nodes (vertices) as shown below. A node was placed at:</p>
<ul>
<li>The start of the course </li>
<li>The end of the course </li>
<li>Every junction </li>
<li>Every potential ground ball location</li>
</ul>
<div class="image">
<img src="arena_caddy.png" alt="arena_caddy.png"/>
<div class="caption">
Caddy's connected graph representation of the arena</div></div>
<p>The software implementation of this graph was done using a C struct:</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span>nodeStruct</div>
<div class="line">{</div>
<div class="line">    uint8_t numAdjNodes;</div>
<div class="line">    uint8_t adjNodes[<a class="code" href="node__list_8h.html#a2fa5ac2ba1926088215fddc03912c90d">MAX_ADJ_NODES</a>];</div>
<div class="line">    uint8_t adjCosts[<a class="code" href="node__list_8h.html#a2fa5ac2ba1926088215fddc03912c90d">MAX_ADJ_NODES</a>];</div>
<div class="line">    int8_t adjHeadings[<a class="code" href="node__list_8h.html#a2fa5ac2ba1926088215fddc03912c90d">MAX_ADJ_NODES</a>];</div>
<div class="line">} NODE;</div>
</div><!-- fragment --><p>Each node defined the adjacent node numbers, directions, and distances to each adjacent node. Headings were defined in units of 8-bit binary radians or "brads". An 8-bit brad is 1/256 of a complete rotation. Brads are particularly useful in that they leverage the inherent rollover digital adders to automatically constrain the range of angular arithmetic operations to [0° 360°). Distances were stored using inches scaled by a factor of 1/6 since most node-to-node distances were multiples of 6 inches. This allowed all distances to be encoded in an 8-bit integer data type.</p>
<p>As an additional measure to conserve memory resources, information about the type of node was encoded in the node number:</p>
<ul>
<li>Node 0: Start </li>
<li>Node 1-20: Ball nodes </li>
<li>Node 21-41: Junction nodes </li>
<li>Node 42: End</li>
</ul>
<p>SRAM memory was particularly limited on our ATMega32 platform, so to conserve it for other uses, the node list was stored in FLASH using a switch statement lookup table. This technique forces the constant values to be encoded in Load Program Memory (LPM) instruction words which are stored and accessed directly from FLASH.</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> getNode(uint8_t nodeNum, NODE *node)</div>
<div class="line">{</div>
<div class="line">    <span class="keywordflow">switch</span> (nodeNum)</div>
<div class="line">    {</div>
<div class="line">    <span class="keywordflow">case</span> 0:                          <span class="comment">// START_NODE</span></div>
<div class="line">        node-&gt;numAdjNodes = 1;</div>
<div class="line">        node-&gt;   adjNodes[0] = 21;</div>
<div class="line">        node-&gt;   adjCosts[0] = 9;</div>
<div class="line">        node-&gt;adjHeadings[0] = -64;</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    <span class="keywordflow">case</span> 1:                          <span class="comment">// First ball node</span></div>
<div class="line">        node-&gt;numAdjNodes = 2;</div>
<div class="line">        node-&gt;   adjNodes[0] = 21;</div>
<div class="line">        node-&gt;   adjNodes[1] = 22;</div>
<div class="line">        node-&gt;   adjCosts[0] = 4;</div>
<div class="line">        node-&gt;   adjCosts[1] = 4;</div>
<div class="line">        node-&gt;adjHeadings[0] = -128;</div>
<div class="line">        node-&gt;adjHeadings[1] = 0;</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line"></div>
<div class="line">    <span class="comment">// ...</span></div>
<div class="line"></div>
<div class="line">    <span class="keywordflow">case</span> 42:                         <span class="comment">// STOP_NODE</span></div>
<div class="line">        node-&gt;numAdjNodes = 1;</div>
<div class="line">        node-&gt;   adjNodes[0] = 41;</div>
<div class="line">        node-&gt;   adjCosts[0] = 5;</div>
<div class="line">        node-&gt;adjHeadings[0] = 0;</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">    }</div>
</div><!-- fragment --><h2><a class="anchor" id="ball_detection"></a>
Ball Detection</h2>
<p>Except for a couple special cases, all ball detection was done at junctions with the tilt servo oriented vertically and the panning servo oriented at a fixed angle to the left or to the right. This covered all junctions in which a ground ball could be located down a perpendicular corridor to the left or right.</p>
<p>The ball detection algorithm was implemented using the built-in color tracking and virtual window features of the CMUcam. As with the line tracking, it was important that all the data processing intensive operations be done on the CMUcam processor in order to achieve low execution time.</p>
<p>At every junction, Caddy traversed all connected nodes in the graph extending to the left, for example. For any ground ball node encountered during this graph traversal that had not yet been checked, the node number was recorded along with distance away from the Caddy's current node location. If there was at least one unchecked ball node to the left, Caddy would orient the camera pan/tilt servos to look in that direction and use a sliding window algorithm to search for the ball.</p>
<p>As shown in the diagram below, the ball detection algorithm searched for the bottom of the ball by progressively moving a narrow image window through the image and using an upper and lower color intensity thresholds to determine if any orange pixels were within the image window.</p>
<div class="image">
<img src="ball_seek_left_view_dia.png" alt="ball_seek_left_view_dia.png"/>
<div class="caption">
The camera's view while searching for a ball down a corridor to the left</div></div>
<p>If an orange pixel was found, a look-up table was used to convert the the horizontal X coordinate of the image window to a physical distance away from the bot. This distance was then compared with the values in the unchecked ball list to find the most likely node number corresponding to the blob of orange pixels.</p>
<div class="image">
<img src="ball_search_flowchart_dia.png" alt="ball_search_flowchart_dia.png"/>
<div class="caption">
Perpendicular corridor ball searching flow chart</div></div>
<p>Since stopping to perform these ball searches added time to the run we made sure to mark any node that was "seen" during a ball search or physically crossed by Caddy as having been checked. Also, once the final ground ball was discovered all remaining unchecked nodes were marked as having been checked (by process of elimination). These optimizations ensured that Caddy only performed ball searches when necessary.</p>
<p>To optimize the computation time of the sliding window search algorithm, the window width was increased from 1 pixel to 4 pixels. We found that this gave sufficient resolution while providing a speedup of nearly 4.0. We also limited the far end of the sliding window range to cover just six inches past the farthest unchecked ball node since there was no point in search through parts of the image which we knew would not offer any new information. With these optimizations a typical ball search to the left or right took about 1 second.</p>
<h2><a class="anchor" id="path_planning"></a>
Path Planning</h2>
<p>The connected graph map of the arena and ball detection features described in the previous sections provided pieces of information needed for a path planning algorithm:</p>
<ul>
<li>A map </li>
<li>A location </li>
<li>One or more destinations (goals)</li>
</ul>
<p>Now, every time a goal was added to the goal list (i.e. a ground ball was found), the path planning algorithm would compute the cost through all permutations of known goals starting from the current node and ending at nest node. This required two sub-algorithms:</p>
<ul>
<li>A method to compute the shortest path between any two nodes </li>
<li>A method to compute all permutations of known goals</li>
</ul>
<p>The permutation algorithm proved to be more challenging than originally expected because the limited stack space of the ATmega32 precluded the use of a recursive permutation algorithm. With some research we found a simple iterative implementation in the GNU C++ Standard Template Library (STL). Adapting this templated code to C99 C code we had a solution for computing all permutations of any array of 8-bit values. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Apr 10 2013 22:02:31 for Caddy by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.1.2
</small></address>
</body>
</html>
